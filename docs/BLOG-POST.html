<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Building Real-Time Speech Translation with AI Avatars Using Azure Speech Services</title>
    <style>
        :root {
            --primary-color: #0078d4;
            --secondary-color: #106ebe;
            --text-color: #333;
            --bg-color: #fff;
            --code-bg: #f4f4f4;
            --border-color: #e1e1e1;
            --blockquote-bg: #f0f7ff;
        }
        
        * {
            box-sizing: border-box;
        }
        
        body {
            font-family: 'Segoe UI', -apple-system, BlinkMacSystemFont, Roboto, 'Helvetica Neue', sans-serif;
            line-height: 1.7;
            color: var(--text-color);
            max-width: 900px;
            margin: 0 auto;
            padding: 40px 20px;
            background-color: var(--bg-color);
        }
        
        h1 {
            font-size: 2.5em;
            color: var(--primary-color);
            margin-bottom: 0.3em;
            line-height: 1.2;
        }
        
        h2 {
            font-size: 1.8em;
            color: var(--secondary-color);
            margin-top: 2em;
            padding-bottom: 0.3em;
            border-bottom: 2px solid var(--border-color);
        }
        
        h3 {
            font-size: 1.4em;
            color: var(--text-color);
            margin-top: 1.5em;
        }
        
        h4 {
            font-size: 1.15em;
            color: var(--text-color);
            margin-top: 1.3em;
        }
        
        .subtitle {
            font-style: italic;
            color: #666;
            font-size: 1.2em;
            margin-bottom: 2em;
        }
        
        hr {
            border: none;
            border-top: 1px solid var(--border-color);
            margin: 2em 0;
        }
        
        p {
            margin: 1em 0;
        }
        
        a {
            color: var(--primary-color);
            text-decoration: none;
        }
        
        a:hover {
            text-decoration: underline;
        }
        
        ul, ol {
            padding-left: 1.5em;
        }
        
        li {
            margin: 0.5em 0;
        }
        
        strong {
            color: #1a1a1a;
        }
        
        blockquote {
            background-color: var(--blockquote-bg);
            border-left: 4px solid var(--primary-color);
            margin: 1.5em 0;
            padding: 1em 1.5em;
            border-radius: 0 8px 8px 0;
        }
        
        blockquote p {
            margin: 0;
        }
        
        code {
            background-color: var(--code-bg);
            padding: 0.2em 0.4em;
            border-radius: 4px;
            font-family: 'Consolas', 'Monaco', 'Courier New', monospace;
            font-size: 0.9em;
        }
        
        pre {
            background-color: #1e1e1e;
            color: #d4d4d4;
            padding: 1.5em;
            border-radius: 8px;
            overflow-x: auto;
            line-height: 1.5;
        }
        
        pre code {
            background-color: transparent;
            padding: 0;
            color: inherit;
        }
        
        .code-comment {
            color: #6a9955;
        }
        
        .code-keyword {
            color: #569cd6;
        }
        
        .code-string {
            color: #ce9178;
        }
        
        .code-function {
            color: #dcdcaa;
        }
        
        .diagram {
            background-color: #f8f9fa;
            border: 1px solid var(--border-color);
            border-radius: 8px;
            padding: 2em;
            margin: 1.5em 0;
            text-align: center;
        }
        
        .diagram-box {
            display: inline-block;
            background-color: #fff;
            border: 2px solid var(--primary-color);
            border-radius: 8px;
            padding: 1em 1.5em;
            margin: 0.5em;
            min-width: 150px;
        }
        
        .diagram-title {
            font-weight: bold;
            color: var(--primary-color);
            margin-bottom: 0.5em;
        }
        
        .diagram-arrow {
            display: inline-block;
            color: #666;
            font-size: 1.5em;
            margin: 0 0.5em;
        }
        
        .section-icon {
            margin-right: 0.3em;
        }
        
        .use-case-section {
            margin: 1.5em 0;
        }
        
        .use-case-title {
            font-weight: bold;
            color: #1a1a1a;
            margin-bottom: 0.3em;
        }
        
        .architecture-flow {
            display: flex;
            flex-wrap: wrap;
            justify-content: center;
            align-items: center;
            gap: 1em;
            margin: 2em 0;
            padding: 2em;
            background: linear-gradient(135deg, #f5f7fa 0%, #c3cfe2 100%);
            border-radius: 12px;
        }
        
        .flow-box {
            background: white;
            border-radius: 10px;
            padding: 1em 1.5em;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
            text-align: center;
            min-width: 140px;
        }
        
        .flow-box-title {
            font-weight: bold;
            color: var(--primary-color);
            font-size: 0.9em;
            margin-bottom: 0.5em;
        }
        
        .flow-box-items {
            font-size: 0.85em;
            color: #666;
        }
        
        .flow-arrow {
            font-size: 1.5em;
            color: var(--primary-color);
        }
        
        .resources-list {
            list-style: none;
            padding-left: 0;
        }
        
        .resources-list li {
            margin: 0.8em 0;
        }
        
        .footer {
            text-align: center;
            margin-top: 3em;
            padding-top: 2em;
            border-top: 1px solid var(--border-color);
            color: #666;
            font-style: italic;
        }
        
        @media (max-width: 600px) {
            body {
                padding: 20px 15px;
            }
            
            h1 {
                font-size: 1.8em;
            }
            
            h2 {
                font-size: 1.4em;
            }
            
            .architecture-flow {
                flex-direction: column;
            }
            
            .flow-arrow {
                transform: rotate(90deg);
            }
        }
    </style>
</head>
<body>
    <article>
        <h1>Building Real-Time Speech Translation with AI Avatars Using Azure Speech Services</h1>
        <p class="subtitle">Breaking language barriers with AI-powered avatars that speak your audience's language</p>
        
        <hr>
        
        <h2>Introduction</h2>
        
        <p>Language barriers remain one of the biggest challenges in communication. Whether you're holding an all-hands meeting for a globally distributed team, consulting with non-native speaking patients, or teaching students across continents‚Äîseamless, real-time translation makes or breaks effective communication.</p>
        
        <p>Traditional translation tools feel impersonal and disconnected. Text captions scroll across screens while speakers continue in their native tongue, creating a disjointed experience. What if your audience could see and hear an AI avatar speaking directly to them in their own language, with natural lip-sync and human-like expressions?</p>
        
        <p><strong>Azure Speech Translation Avatar</strong> is designed to address these requirements: a speaker talks in one language, and listeners watch an AI avatar deliver the translated speech in their chosen language.</p>
        
        <p>Imagine a CEO in Tokyo delivering a quarterly update. Employees in Munich, S√£o Paulo, and Mumbai each see an AI avatar speaking to them in German, Portuguese, and Hindi respectively‚Äîall in real-time, with synchronized lip movements and natural speech patterns. The speaker focuses on their message; the technology handles the rest.</p>
        
        <blockquote>
            <p>üíª <strong>Source Code</strong>: The complete implementation is available on GitHub: <a href="https://github.com/l-sudarsan/avatar-translation">avatar-translation</a></p>
        </blockquote>
        
        <hr>
        
        <h2>How It Works</h2>
        
        <p>The application uses a <strong>session-based Speaker/Listener architecture</strong> to separate the presenter's control interface from the audience's viewing experience. This design prevents audio feedback loops and optimizes the user experience for each role.</p>
        
        <h3>Speaker Mode</h3>
        
        <p>The speaker interface gives presenters full control over the translation session:</p>
        
        <ul>
            <li><strong>Session Management</strong>: Create sessions and generate shareable listener URLs</li>
            <li><strong>Language Configuration</strong>: Select source language (what you speak) and target language (what listeners hear)</li>
            <li><strong>Avatar Selection</strong>: Choose from prebuilt or custom avatars for the translation output</li>
            <li><strong>Real-time Feedback</strong>: View live transcription of your speech and monitor listener count</li>
            <li><strong>No Avatar Display</strong>: The interface intentionally hides the avatar video/audio to prevent microphone feedback loops</li>
        </ul>
        
        <h3>Listener Mode</h3>
        
        <p>The listener interface delivers an immersive, distraction-free viewing experience:</p>
        
        <ul>
            <li><strong>Easy Access</strong>: Join via a simple URL containing the session code (e.g., <code>/listener/123456</code>)</li>
            <li><strong>Avatar Video</strong>: Watch the AI avatar with synchronized lip movements matching the translated speech</li>
            <li><strong>Translated Audio</strong>: Hear the avatar speak the translation in the target language</li>
            <li><strong>Caption Display</strong>: Read real-time translation text alongside the avatar</li>
            <li><strong>Translation History</strong>: Scroll through all translations from the session</li>
            <li><strong>No Controls</strong>: Focus entirely on the content without distracting UI elements</li>
        </ul>
        
        <h3>Technology Stack</h3>
        
        <p>The diagram below shows how the components interact. The Flask server acts as the central hub, coordinating communication between the speaker's browser, Azure Speech Services, and multiple listener clients.</p>
        
        <div class="architecture-flow">
            <div class="flow-box">
                <div class="flow-box-title">üé§ SPEAKER</div>
                <div class="flow-box-items">Microphone<br>Controls<br>Transcription</div>
            </div>
            <div class="flow-arrow">‚Üí</div>
            <div class="flow-box">
                <div class="flow-box-title">üñ•Ô∏è FLASK SERVER</div>
                <div class="flow-box-items">Session Management<br>Translation<br>Broadcasting</div>
            </div>
            <div class="flow-arrow">‚Üî</div>
            <div class="flow-box">
                <div class="flow-box-title">‚òÅÔ∏è AZURE SPEECH</div>
                <div class="flow-box-items">Speech Recognition<br>Translation<br>Avatar Synthesis</div>
            </div>
            <div class="flow-arrow">‚Üí</div>
            <div class="flow-box">
                <div class="flow-box-title">üë• LISTENERS</div>
                <div class="flow-box-items">Avatar Video<br>Live Captions<br>Audio Output</div>
            </div>
        </div>
        
        <hr>
        
        <h2>Implementation Deep Dive</h2>
        
        <p>You can check the complete source code in the <a href="https://github.com/l-sudarsan/avatar-translation">GitHub repository</a>.</p>
        
        <h3>Core Components</h3>
        
        <p>Five main technical components power the application, each handling a specific part of the translation pipeline.</p>
        
        <h4>1. Backend: Flask + Socket.IO</h4>
        
        <p>The server uses <strong>Flask</strong> and <strong>Flask-SocketIO</strong> with the <strong>Eventlet</strong> async worker for WebSocket support. This combination delivers:</p>
        
        <ul>
            <li><strong>HTTP endpoints</strong> for session management and avatar connection</li>
            <li><strong>WebSocket rooms</strong> for real-time translation broadcasting</li>
            <li><strong>Session storage</strong> for managing multiple concurrent translation sessions</li>
        </ul>
        
<pre><code><span class="code-comment"># Session structure</span>
sessions = {
    <span class="code-string">"123456"</span>: {
        <span class="code-string">"name"</span>: <span class="code-string">"Q1 Townhall"</span>,
        <span class="code-string">"source_language"</span>: <span class="code-string">"en-US"</span>,
        <span class="code-string">"target_language"</span>: <span class="code-string">"ja-JP"</span>,
        <span class="code-string">"avatar"</span>: <span class="code-string">"lisa"</span>,
        <span class="code-string">"listeners"</span>: <span class="code-keyword">set</span>(),
        <span class="code-string">"is_translating"</span>: <span class="code-keyword">False</span>
    }
}</code></pre>
        
        <h4>2. Audio Streaming: Browser to Server</h4>
        
        <p>Instead of relying on server-side microphone access, the browser captures audio directly using the <strong>Web Audio API</strong>:</p>
        
<pre><code><span class="code-comment">// Speaker captures microphone at 16kHz</span>
<span class="code-keyword">const</span> audioContext = <span class="code-keyword">new</span> <span class="code-function">AudioContext</span>({ sampleRate: <span class="code-string">16000</span> });
<span class="code-keyword">const</span> mediaStream = <span class="code-keyword">await</span> navigator.mediaDevices.<span class="code-function">getUserMedia</span>({ audio: <span class="code-keyword">true</span> });

<span class="code-comment">// Process audio and send via Socket.IO</span>
processor.onaudioprocess = (event) => {
    <span class="code-keyword">const</span> pcmData = <span class="code-function">convertToPCM16</span>(event.inputBuffer);
    socket.<span class="code-function">emit</span>(<span class="code-string">'audioData'</span>, { sessionId, audioData: pcmData });
};</code></pre>
        
        <p>This approach works seamlessly across different deployment environments without requiring server microphone permissions.</p>
        
        <h4>3. Azure Speech Translation</h4>
        
        <p>The server receives audio chunks and feeds them to Azure's <strong>TranslationRecognizer</strong> via a <strong>PushAudioInputStream</strong>:</p>
        
<pre><code><span class="code-comment"># Configure translation</span>
translation_config = speechsdk.translation.<span class="code-function">SpeechTranslationConfig</span>(
    subscription=SPEECH_KEY,
    region=SPEECH_REGION
)
translation_config.speech_recognition_language = <span class="code-string">"en-US"</span>
translation_config.<span class="code-function">add_target_language</span>(<span class="code-string">"ja"</span>)

<span class="code-comment"># Push audio stream</span>
push_stream = speechsdk.audio.<span class="code-function">PushAudioInputStream</span>()
audio_config = speechsdk.audio.<span class="code-function">AudioConfig</span>(stream=push_stream)

<span class="code-comment"># Handle recognition results</span>
<span class="code-keyword">def</span> <span class="code-function">on_recognized</span>(evt):
    translation = evt.result.translations[<span class="code-string">"ja"</span>]
    socketio.<span class="code-function">emit</span>(<span class="code-string">'translationResult'</span>, {
        <span class="code-string">'original'</span>: evt.result.text,
        <span class="code-string">'translated'</span>: translation
    }, room=session_id)</code></pre>
        
        <h4>4. Avatar Synthesis with WebRTC</h4>
        
        <p>Each listener establishes a <strong>WebRTC</strong> connection to Azure's Avatar Service:</p>
        
        <ol>
            <li><strong>ICE Token Exchange</strong>: Server provides TURN server credentials</li>
            <li><strong>SDP Negotiation</strong>: Browser and Azure exchange session descriptions</li>
            <li><strong>Avatar Connection</strong>: Listener sends local SDP offer, receives remote answer</li>
            <li><strong>Video Stream</strong>: Avatar video flows directly to listener via WebRTC</li>
        </ol>
        
<pre><code><span class="code-comment">// Listener connects to avatar</span>
<span class="code-keyword">const</span> peerConnection = <span class="code-keyword">new</span> <span class="code-function">RTCPeerConnection</span>(iceConfig);
<span class="code-keyword">const</span> offer = <span class="code-keyword">await</span> peerConnection.<span class="code-function">createOffer</span>();
<span class="code-keyword">await</span> peerConnection.<span class="code-function">setLocalDescription</span>(offer);

<span class="code-comment">// Send to Azure Avatar Service</span>
<span class="code-keyword">const</span> response = <span class="code-keyword">await</span> <span class="code-function">fetch</span>(<span class="code-string">'/api/connectListenerAvatar'</span>, {
    method: <span class="code-string">'POST'</span>,
    headers: { <span class="code-string">'session-id'</span>: sessionId },
    body: JSON.<span class="code-function">stringify</span>({ sdp: offer.sdp })
});

<span class="code-keyword">const</span> { sdp: remoteSdp } = <span class="code-keyword">await</span> response.<span class="code-function">json</span>();
<span class="code-keyword">await</span> peerConnection.<span class="code-function">setRemoteDescription</span>({ type: <span class="code-string">'answer'</span>, sdp: remoteSdp });</code></pre>
        
        <h4>5. Real-Time Broadcasting</h4>
        
        <p>When the speaker talks, translations flow to all listeners simultaneously:</p>
        
        <div class="architecture-flow" style="padding: 1.5em;">
            <div class="flow-box" style="min-width: 100px;">üé§ Speaker speaks</div>
            <div class="flow-arrow">‚Üí</div>
            <div class="flow-box" style="min-width: 100px;">Audio sent to server</div>
            <div class="flow-arrow">‚Üí</div>
            <div class="flow-box" style="min-width: 100px;">Azure translates</div>
            <div class="flow-arrow">‚Üí</div>
            <div class="flow-box" style="min-width: 100px;">Server broadcasts</div>
            <div class="flow-arrow">‚Üí</div>
            <div class="flow-box" style="min-width: 100px;">üó£Ô∏è Avatar speaks</div>
        </div>
        
        <p>Each listener maintains their own WebRTC connection to the Avatar Service, ensuring independent video streams while receiving synchronized translation text.</p>
        
        <h3>Key Design Decisions</h3>
        
        <ul>
            <li><strong>Browser audio capture</strong>: Works in any environment without requiring server microphone permissions</li>
            <li><strong>Session-based rooms</strong>: Isolates translation streams and supports multiple concurrent sessions</li>
            <li><strong>Separate speaker/listener UIs</strong>: Prevents audio feedback and optimizes each user's experience</li>
            <li><strong>Socket.IO for broadcasts</strong>: Delivers reliable real-time messaging with automatic reconnection</li>
            <li><strong>WebRTC for avatar</strong>: Provides low-latency video streaming with peer-to-peer efficiency</li>
        </ul>
        
        <hr>
        
        <h2>Application Areas</h2>
        
        <p>Real-time speech translation with AI avatars unlocks transformative possibilities across industries. Here are key sectors where this technology drives significant impact.</p>
        
        <h3><span class="section-icon">üè¢</span> Enterprise &amp; Corporate</h3>
        
        <div class="use-case-section">
            <p class="use-case-title">Internal Townhalls &amp; All-Hands Meetings</p>
            <p>Global organizations deliver executive communications where every employee hears the message in their native language‚Äînot through subtitles, but through an avatar speaking directly to them.</p>
        </div>
        
        <div class="use-case-section">
            <p class="use-case-title">Sales Conversations</p>
            <p>Sales teams engage international prospects without language barriers. The avatar builds a more personal connection than text translation while preserving the original speaker's authenticity.</p>
        </div>
        
        <div class="use-case-section">
            <p class="use-case-title">Training &amp; Onboarding</p>
            <p>Standardized training content reaches employees worldwide, with each viewer experiencing the material in their preferred language through an engaging avatar presenter.</p>
        </div>
        
        <h3><span class="section-icon">üè•</span> Healthcare</h3>
        
        <div class="use-case-section">
            <p class="use-case-title">Patient Communication</p>
            <p>Healthcare providers consult with patients who speak different languages, while the avatar delivers medical information clearly and accurately in the patient's native tongue.</p>
        </div>
        
        <div class="use-case-section">
            <p class="use-case-title">Telehealth</p>
            <p>Remote healthcare consultations reach non-native speakers effectively, improving health outcomes by ensuring patients fully understand their care instructions.</p>
        </div>
        
        <h3><span class="section-icon">üéì</span> Education</h3>
        
        <div class="use-case-section">
            <p class="use-case-title">Online Learning</p>
            <p>Educational institutions expand their global reach, offering lectures and courses in multiple languages through avatar presenters.</p>
        </div>
        
        <div class="use-case-section">
            <p class="use-case-title">Interactive Lessons</p>
            <p>Engaging avatar presenters captivate students while delivering content in their native language.</p>
        </div>
        
        <div class="use-case-section">
            <p class="use-case-title">Museum Tours</p>
            <p>Cultural institutions offer multilingual guided experiences where visitors receive personalized tours in their language of choice.</p>
        </div>
        
        <h3><span class="section-icon">üì∫</span> Media &amp; Entertainment</h3>
        
        <div class="use-case-section">
            <p class="use-case-title">Broadcasting</p>
            <p>News organizations and content creators deliver content to international audiences with localized avatar presenters, keeping viewers engaged while breaking language barriers.</p>
        </div>
        
        <div class="use-case-section">
            <p class="use-case-title">Live Events</p>
            <p>Conferences, product launches, and presentations reach global audiences with real-time translated avatar streams for each language group.</p>
        </div>
        
        <hr>
        
        <h2>Custom Avatars: Your Brand, Your Voice</h2>
        
        <p>While prebuilt avatars work great for many scenarios, organizations can build <strong>custom avatars</strong> that represent their brand identity. This section covers the creation process and important ethical considerations.</p>
        
        <h3>The Process</h3>
        
        <ol>
            <li><strong>Request Access</strong>: Submit <a href="https://aka.ms/customneural">Microsoft's intake form</a> for custom avatar approval</li>
            <li><strong>Record Training Data</strong>: Capture at least 10 minutes of video featuring your avatar talent</li>
            <li><strong>Obtain Consent</strong>: Record the talent acknowledging use of their likeness</li>
            <li><strong>Train the Model</strong>: Use Microsoft Foundry Portal to train your custom avatar</li>
            <li><strong>Deploy</strong>: Deploy the trained model to your Azure Speech resource</li>
        </ol>
        
        <h3>Responsible AI Considerations</h3>
        
        <p>Building synthetic representations of people carries ethical responsibilities:</p>
        
        <ul>
            <li><strong>Explicit Written Consent</strong>: Always get permission from the talent</li>
            <li><strong>Informed Consent</strong>: Make sure talent understands how the technology works</li>
            <li><strong>Usage Transparency</strong>: Share intended use cases with the talent</li>
            <li><strong>Prohibited Uses</strong>: Never use for deception, misinformation, or impersonation</li>
        </ul>
        
        <p>Microsoft publishes comprehensive <a href="https://learn.microsoft.com/en-us/azure/ai-foundry/responsible-ai/speech-service/text-to-speech/disclosure-voice-talent">Responsible AI guidelines</a> that you must follow when creating custom avatars.</p>
        
        <hr>
        
        <h2>Getting Started</h2>
        
        <p>Ready to build your own real-time translation avatar application? Grab the complete source code and documentation from GitHub.</p>
        
        <blockquote>
            <p>üìö <strong>Full Documentation</strong>: <a href="https://github.com/l-sudarsan/avatar-translation/tree/master/docs">github.com/l-sudarsan/avatar-translation/docs</a></p>
        </blockquote>
        
        <h3>Prerequisites</h3>
        <ul>
            <li>Python 3.8+</li>
            <li>Azure Speech Service subscription</li>
            <li>Modern browser (Chrome, Edge, Firefox)</li>
        </ul>
        
        <h3>Quick Start</h3>
        
<pre><code><span class="code-comment"># Clone the repository</span>
git clone https://github.com/l-sudarsan/avatar-translation.git
cd avatar-translation

<span class="code-comment"># 1. Create and activate virtual environment</span>
python -m venv venv
.\venv\Scripts\Activate

<span class="code-comment"># 2. Install dependencies</span>
pip install -r requirements.txt

<span class="code-comment"># 3. Configure Azure credentials</span>
cp .env.example .env
<span class="code-comment"># Edit .env with your SPEECH_REGION and SPEECH_KEY</span>

<span class="code-comment"># 4. Run the application</span>
python -m flask run --host=0.0.0.0 --port=5000</code></pre>
        
        <h3>Demo Sequence</h3>
        
        <ol>
            <li>Open <code>http://localhost:5000/speaker</code></li>
            <li>Configure session (name, source language, target language, avatar)</li>
            <li>Click <strong>Create Session</strong> ‚Üí Copy the listener URL</li>
            <li>Open the listener URL in another browser/device</li>
            <li><strong>Wait</strong> for the avatar to connect (video appears)</li>
            <li>Start speaking ‚Üí Listeners see the avatar + translations</li>
        </ol>
        
        <blockquote>
            <p><strong>Tip</strong>: For the best demo experience, open the listener URL on a separate device to avoid audio feedback from the avatar's output being picked up by the speaker's microphone.</p>
        </blockquote>
        
        <hr>
        
        <h2>Conclusion</h2>
        
        <p>Real-time speech translation with AI avatars marks a significant leap forward in cross-language communication. By combining Azure's powerful Speech Translation, Text-to-Speech, and Avatar Synthesis services, you can build experiences that feel personal and engaging‚Äînot just functional.</p>
        
        <p>The speaker/listener architecture cleanly separates concerns: speakers focus on their message while listeners receive a tailored, localized experience. WebRTC delivers low-latency video streaming, Socket.IO handles real-time translation broadcasting, and Azure powers the complex AI processing.</p>
        
        <p>Whether you're a global enterprise connecting your workforce, a healthcare provider serving diverse communities, or an educator reaching international students‚Äîthis technology makes truly inclusive communication possible.</p>
        
        <p><strong>The future of multilingual communication isn't about reading subtitles. It's about having someone speak directly to you in your language.</strong></p>
        
        <hr>
        
        <h2>Resources</h2>
        
        <h3>Project Repository</h3>
        <ul class="resources-list">
            <li>üíª <a href="https://github.com/l-sudarsan/avatar-translation">GitHub: avatar-translation</a></li>
            <li>üìö <a href="https://github.com/l-sudarsan/avatar-translation/tree/master/docs">Project Documentation</a></li>
        </ul>
        
        <h3>Azure Documentation</h3>
        <ul class="resources-list">
            <li><a href="https://learn.microsoft.com/en-us/azure/ai-services/speech-service/">Azure Speech Service Documentation</a></li>
            <li><a href="https://learn.microsoft.com/en-us/azure/ai-services/speech-service/text-to-speech-avatar/what-is-text-to-speech-avatar">Text-to-Speech Avatar Overview</a></li>
            <li><a href="https://learn.microsoft.com/en-us/azure/ai-services/speech-service/text-to-speech-avatar/custom-avatar-create">Custom Avatar Creation Guide</a></li>
            <li><a href="https://learn.microsoft.com/en-us/azure/ai-foundry/responsible-ai/speech-service/text-to-speech/disclosure-voice-talent">Responsible AI for Voice &amp; Avatar</a></li>
            <li><a href="https://learn.microsoft.com/en-us/azure/ai-services/speech-service/language-support?tabs=speech-translation">Language Support</a></li>
        </ul>
        
        <div class="footer">
            <p>Built with Azure AI Services, GitHub Copilot, Flask, Socket.IO, and WebRTC.</p>
        </div>
    </article>
</body>
</html>
